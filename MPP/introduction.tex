\section{Introduction}
  As physical constraints make it increasingly difficult to continue the trend of increasing clock speed, hardware manufacturers are responding by adding more cores to \ac{CPU} chips in order to continue providing improvements to the rate at which instructions can be executed. \cite{perf}
  Each iteration of desktop processor seems to increase the number of hardware threads available; however, the increased core count of a modern \ac{CPU} is still far lower than that of those found in \ac{GPUs}.

\ac{GPUs} are highly parallel co-processors designed for \ac{SIMD} execution on a vector dataset.
\ac{GPUs} traditionally use hundreds or thousands of shader units to perform functions required to render a 3D scene; though recently, frameworks such as \ac{OpenCL} have facilitated the usage of shader units to perform arbitrary tasks specified by a subset of C code.

 With the capability of user-defined code execution on each GPU core, devices often purchased for recreational purposes gain theoretical data-processing capabilities that far exceed those of systems solely scheduling instructions on a multi-core \ac{CPU}. \cite{mars}

Unfortunately, simply adding more cores in order to increase the speed at which a system can perform computation provides gains that cannot be utilised fully by common sequential programming approaches.
In order to exploit the full potential for multiple threads to be executed concurrently, the programmer needs to structure data as a collection of processable entities that share no dependencies.
Any calculations within a thread that require knowledge of the state of other threads are unable to continue until the shared state can be synchronised.

In addition to the locality-dependant penalty of memory synchronisation, due to the nature of \ac{SIMD} execution that runs in lockstep, code written for \ac{GPUs} becomes inefficient if the work kernels contain significant branching. \cite{branching}
These disadvantages, force programmers exploring the boundaries of performance to adapt new paradigms and data-flow models when solving otherwise familiar problems.

\emph{Functional Programming} language-features, and those inspired by them, often offer advantages when exploring parallel execution \cite{parallelfunction} due to abstraction hiding the concepts of \emph{state} and \emph{mutable data}.
Pure functional programming provides \emph{referential transparency} as each function can be replaced by just its result without affecting the correctness of the program.
The ability to simplify long chains of computation into a series of values being mapped to outputs greatly increases the ease of verifying an algorithm, as well as highlighting computations that cannot affect the result of others and can therefore be run concurrently.

The purpose of this project is to combine the benefits provided by functional-inspired paradigms with the increased theoretical performance of \ac{GPUs} in order to provide an easily-utilised library for parallelisation.

Each feature implemented shall be evaluated against existing implementations of functional languages and on similar GPU/\ac{OpenCL} frameworks.
